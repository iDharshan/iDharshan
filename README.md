# Hey, I'm Dharshan 👋  
AI Engineer (who builds stuff that *actually* works)

## So, what do I do?
- Build and deploy ML models that make sense (and predictions)
- Fine-tune LLMs using LoRA, PEFT — the good stuff, not the buzzwords
- RAG pipelines with LangChain + vector DBs that *actually* retrieve relevant info
- Wrap it all up with FastAPI, Docker, and a touch of ✨ sanity ✨

## What went wrong (and what I learned):
- **Started training transformers before understanding attention**  
  → Drowned in tensor hell. Came back knowing exactly how and why attention works.

- **Overengineered LSTMs with too many layers**  
  → Thought “deeper = better.” Learned that simplicity + the right loss function > 50 layers of pain.

- **Assumed FAISS “just works”**  
  → Spoiler: it doesn’t. Learned to tune it, index properly, and *actually* measure retrieval quality.

- **Wrote messy APIs during my first FastAPI project**  
  → Came back with better routing, modular structure, and swagger docs that don’t scream "help me."

## Cool stuff I've worked on:
- Transformers from scratch — NumPy to PyTorch (yes, it actually worked)
- LSTM models for real-world problems (SRU optimization, aquaponics monitoring)
- RAG systems + LLM fine-tuning in freelance projects (and nobody cried)
- Currently building on Jetson Orin Nano for real-time scrap detection (fancy, I know)

## Tech I actually use:
`PyTorch` • `FastAPI` • `scikit-learn` • `NumPy` • `Pandas` • `LangChain`• `Azure`

> I don’t just read AI papers — I turn them into working code (and clean up after the explosion).


