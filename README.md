# Hey, I'm Dharshan ðŸ‘‹  
AI Engineer (who builds stuff that *actually* works)

## So, what do I do?
- Build and deploy ML models that make sense (and predictions)
- Fine-tune LLMs using LoRA, PEFT â€” the good stuff, not the buzzwords
- RAG pipelines with LangChain + vector DBs that *actually* retrieve relevant info
- Wrap it all up with FastAPI, Docker, and a touch of âœ¨ sanity âœ¨

## What went wrong (and what I learned):
- **Started training transformers before understanding attention**  
  â†’ Drowned in tensor hell. Came back knowing exactly how and why attention works.

- **Overengineered LSTMs with too many layers**  
  â†’ Thought â€œdeeper = better.â€ Learned that simplicity + the right loss function > 50 layers of pain.

- **Assumed FAISS â€œjust worksâ€**  
  â†’ Spoiler: it doesnâ€™t. Learned to tune it, index properly, and *actually* measure retrieval quality.

- **Wrote messy APIs during my first FastAPI project**  
  â†’ Came back with better routing, modular structure, and swagger docs that donâ€™t scream "help me."

## Cool stuff I've worked on:
- Transformers from scratch â€” NumPy to PyTorch (yes, it actually worked)
- LSTM models for real-world problems (SRU optimization, aquaponics monitoring)
- RAG systems + LLM fine-tuning in freelance projects (and nobody cried)
- Currently building on Jetson Orin Nano for real-time scrap detection (fancy, I know)

## Tech I actually use:
`PyTorch` â€¢ `FastAPI` â€¢ `scikit-learn` â€¢ `NumPy` â€¢ `Pandas` â€¢ `LangChain`â€¢ `Azure`

> I donâ€™t just read AI papers â€” I turn them into working code (and clean up after the explosion).


